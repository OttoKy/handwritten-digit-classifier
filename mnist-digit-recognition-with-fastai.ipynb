{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-02-15T01:42:51.048025Z","iopub.status.busy":"2023-02-15T01:42:51.047350Z","iopub.status.idle":"2023-02-15T01:43:12.129720Z","shell.execute_reply":"2023-02-15T01:43:12.128069Z","shell.execute_reply.started":"2023-02-15T01:42:51.047974Z"},"trusted":true},"outputs":[],"source":["!pip install -Uqq fastbook\n","import fastbook\n","from fastai.vision.all import *\n","from fastbook import *\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font size=\"4\">In this project, I built a deep learning model for the MNIST dataset using fastai and PyTorch. I modified the data, implemented loss and accuracy functions, and gained experience in deep learning.</font>\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:22:58.499774Z","iopub.status.busy":"2023-02-15T02:22:58.498435Z","iopub.status.idle":"2023-02-15T02:22:58.507110Z","shell.execute_reply":"2023-02-15T02:22:58.505421Z","shell.execute_reply.started":"2023-02-15T02:22:58.499717Z"},"trusted":true},"outputs":[],"source":["path = untar_data(URLs.MNIST)\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:01.010395Z","iopub.status.busy":"2023-02-15T02:23:01.009978Z","iopub.status.idle":"2023-02-15T02:23:01.016229Z","shell.execute_reply":"2023-02-15T02:23:01.014999Z","shell.execute_reply.started":"2023-02-15T02:23:01.010362Z"},"trusted":true},"outputs":[],"source":["Path.BASE_PATH = path\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"4\">This code block reads the images and labels from the training directory and stores them in a format suitable for machine learning algorithms.</font>"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:02.859640Z","iopub.status.busy":"2023-02-15T02:23:02.858780Z","iopub.status.idle":"2023-02-15T02:23:21.721829Z","shell.execute_reply":"2023-02-15T02:23:21.720836Z","shell.execute_reply.started":"2023-02-15T02:23:02.859598Z"},"trusted":true},"outputs":[],"source":["train_x = []\n","train_y = []\n","labels = os.listdir(path/'training')\n","labels.sort()\n","for label in labels:\n","    for file in (path/'training'/label).ls().sorted():\n","        image = Image.open(file)\n","        train_x.append(tensor(image))\n","        train_y.append(int(label))\n"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4>This code block splits the dataset into training and validation sets, preprocesses the image and label data, and ensures the label tensors have the correct dimensions for use in the model.<font size>"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:24.097338Z","iopub.status.busy":"2023-02-15T02:23:24.096468Z","iopub.status.idle":"2023-02-15T02:23:24.573954Z","shell.execute_reply":"2023-02-15T02:23:24.572875Z","shell.execute_reply.started":"2023-02-15T02:23:24.097296Z"},"trusted":true},"outputs":[],"source":["splitter = RandomSplitter(valid_pct=0.2)\n","train_idx, valid_idx = splitter(range(len(train_x)))\n","\n","x_train = [train_x[i] for i in train_idx]\n","y_train = [train_y[i] for i in train_idx]\n","x_valid = [train_x[i] for i in valid_idx]\n","y_valid = [train_y[i] for i in valid_idx]\n","\n","x_train = (torch.stack(x_train).float()/255).view(-1,28*28)\n","y_train = tensor(y_train).view(-1,1)\n","x_valid = (torch.stack(x_valid).float()/255).view(-1,28*28)\n","y_valid = tensor(y_valid).view(-1,1)\n","\n","x_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n","\n","y_train = y_train.squeeze()\n","y_valid = y_valid.squeeze()"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4>Combining the inputs and labels for the training dataset into tuples, and creating a list of these tuples.<font size>"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:27.324480Z","iopub.status.busy":"2023-02-15T02:23:27.324066Z","iopub.status.idle":"2023-02-15T02:23:27.964365Z","shell.execute_reply":"2023-02-15T02:23:27.963001Z","shell.execute_reply.started":"2023-02-15T02:23:27.324436Z"},"trusted":true},"outputs":[],"source":["training = list(zip(x_train, y_train))\n","validation = list(zip(x_valid, y_valid))\n"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4> This function calculates the cross entropy loss between the predicted values and the actual labels. First, we apply the log softmax function to the predicted values. Then we calculate the negative log likelihood loss using the actual labels. This loss penalizes the model more when it makes more confident incorrect predictions, and less when it makes less confident incorrect predictions. Lastly we return loss value. <font size>\n","    "]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:30.144744Z","iopub.status.busy":"2023-02-15T02:23:30.144163Z","iopub.status.idle":"2023-02-15T02:23:30.151933Z","shell.execute_reply":"2023-02-15T02:23:30.150607Z","shell.execute_reply.started":"2023-02-15T02:23:30.144695Z"},"trusted":true},"outputs":[],"source":["def cross_entropy_loss(predictions, train_y):\n","    log_softmax = F.log_softmax(predictions, dim=1)\n","    loss = F.nll_loss(log_softmax, train_y)\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4> This function calculates the accuracy of the predicted labels for a batch of inputs \n","    by comparing them to the actual target labels. Then function first computes the predicted labels by taking the argmax over the output predictions. It then compares these predicted labels to the actual labels and returns the mean accuracy over the batch. <font size>"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:33.336193Z","iopub.status.busy":"2023-02-15T02:23:33.335609Z","iopub.status.idle":"2023-02-15T02:23:33.343590Z","shell.execute_reply":"2023-02-15T02:23:33.341790Z","shell.execute_reply.started":"2023-02-15T02:23:33.336143Z"},"trusted":true},"outputs":[],"source":["def batch_accuracy(preds, targets):\n","    preds = torch.argmax(preds, dim=1)\n","    return (preds == targets).float().mean()"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4> Creating PyTorch/FastAI data loaders for our training and validation sets. <font size>"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:23:36.014448Z","iopub.status.busy":"2023-02-15T02:23:36.013280Z","iopub.status.idle":"2023-02-15T02:23:36.024672Z","shell.execute_reply":"2023-02-15T02:23:36.023259Z","shell.execute_reply.started":"2023-02-15T02:23:36.014402Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(fastai.data.load.DataLoader,\n"," fastai.data.load.DataLoader,\n"," fastai.data.core.DataLoaders)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["dl = DataLoader(training, batch_size=256)\n","valid_dl = DataLoader(validation, batch_size=256)\n","dls = DataLoaders(dl, valid_dl)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4> Creating feedforward neural network with ReLU activation functions <font size>"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:33:20.708420Z","iopub.status.busy":"2023-02-15T02:33:20.707800Z","iopub.status.idle":"2023-02-15T02:33:20.721821Z","shell.execute_reply":"2023-02-15T02:33:20.720520Z","shell.execute_reply.started":"2023-02-15T02:33:20.708368Z"},"trusted":true},"outputs":[],"source":["simple_net = nn.Sequential(\n","    nn.Linear(28*28,100),\n","    nn.ReLU(),\n","    nn.Linear(100,50),\n","    nn.ReLU(),\n","    nn.Linear(50,10)\n","   \n",")"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4> Creating FastAI learner with \"our\" neural network, loss function and batch accuracy. Using SGD for optimizing.<font size>"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:33:25.700113Z","iopub.status.busy":"2023-02-15T02:33:25.699643Z","iopub.status.idle":"2023-02-15T02:33:25.707553Z","shell.execute_reply":"2023-02-15T02:33:25.706164Z","shell.execute_reply.started":"2023-02-15T02:33:25.700076Z"},"trusted":true},"outputs":[],"source":["learn = Learner(dls, simple_net, opt_func=SGD,\n","                loss_func=cross_entropy_loss, metrics=batch_accuracy,)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=4> And then training with 0.1 lr <font size>\n","    "]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T02:33:28.584746Z","iopub.status.busy":"2023-02-15T02:33:28.583481Z","iopub.status.idle":"2023-02-15T02:33:52.834871Z","shell.execute_reply":"2023-02-15T02:33:52.833496Z","shell.execute_reply.started":"2023-02-15T02:33:28.584688Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>batch_accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.670606</td>\n","      <td>0.601179</td>\n","      <td>0.794167</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.374279</td>\n","      <td>0.445963</td>\n","      <td>0.859667</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.309616</td>\n","      <td>0.373854</td>\n","      <td>0.882750</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.272211</td>\n","      <td>0.324112</td>\n","      <td>0.899000</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.243279</td>\n","      <td>0.281811</td>\n","      <td>0.913917</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.218369</td>\n","      <td>0.248300</td>\n","      <td>0.926583</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.197272</td>\n","      <td>0.223467</td>\n","      <td>0.933500</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.179384</td>\n","      <td>0.204190</td>\n","      <td>0.937250</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.164212</td>\n","      <td>0.187359</td>\n","      <td>0.943333</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.151023</td>\n","      <td>0.173701</td>\n","      <td>0.947917</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.139607</td>\n","      <td>0.162122</td>\n","      <td>0.951167</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.129605</td>\n","      <td>0.151907</td>\n","      <td>0.954167</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.120550</td>\n","      <td>0.142938</td>\n","      <td>0.956750</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.112518</td>\n","      <td>0.135233</td>\n","      <td>0.958667</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.105157</td>\n","      <td>0.128735</td>\n","      <td>0.960250</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.098466</td>\n","      <td>0.123318</td>\n","      <td>0.962417</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.092389</td>\n","      <td>0.118607</td>\n","      <td>0.963583</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.086778</td>\n","      <td>0.114256</td>\n","      <td>0.964333</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.081637</td>\n","      <td>0.110838</td>\n","      <td>0.965833</td>\n","      <td>00:01</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.076934</td>\n","      <td>0.107942</td>\n","      <td>0.967167</td>\n","      <td>00:01</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["learn.fit(20, 0.1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font size=4> The goal of this small project was to gain a deeper understanding of key concepts in deep learning such as tensors and data, Stochastic Gradient Descent (SGD), loss functions, neural networks, and batches.\n"," Mainly using material from fastbook [https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb] <font size>"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
