{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqq fastbook\nimport fastbook\nfrom fastai.vision.all import *\nfrom fastbook import *\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-15T01:42:51.047350Z","iopub.execute_input":"2023-02-15T01:42:51.048025Z","iopub.status.idle":"2023-02-15T01:43:12.129720Z","shell.execute_reply.started":"2023-02-15T01:42:51.047974Z","shell.execute_reply":"2023-02-15T01:43:12.128069Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\">In this project, I built a deep learning model for the MNIST dataset using fastai and PyTorch. I modified the data, implemented \"custom\" loss and accuracy functions, and gained experience in deep learning and data analysis.</font>\n","metadata":{}},{"cell_type":"code","source":"path = untar_data(URLs.MNIST)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:22:58.498435Z","iopub.execute_input":"2023-02-15T02:22:58.499774Z","iopub.status.idle":"2023-02-15T02:22:58.507110Z","shell.execute_reply.started":"2023-02-15T02:22:58.499717Z","shell.execute_reply":"2023-02-15T02:22:58.505421Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"Path.BASE_PATH = path\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:01.009978Z","iopub.execute_input":"2023-02-15T02:23:01.010395Z","iopub.status.idle":"2023-02-15T02:23:01.016229Z","shell.execute_reply.started":"2023-02-15T02:23:01.010362Z","shell.execute_reply":"2023-02-15T02:23:01.014999Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\">This code block reads the images and labels from the training directory and stores them in a format suitable for machine learning algorithms.</font>","metadata":{}},{"cell_type":"code","source":"train_x = []\ntrain_y = []\nlabels = os.listdir(path/'training')\nlabels.sort()\nfor label in labels:\n    for file in (path/'training'/label).ls().sorted():\n        image = Image.open(file)\n        train_x.append(tensor(image))\n        train_y.append(int(label))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:02.858780Z","iopub.execute_input":"2023-02-15T02:23:02.859640Z","iopub.status.idle":"2023-02-15T02:23:21.721829Z","shell.execute_reply.started":"2023-02-15T02:23:02.859598Z","shell.execute_reply":"2023-02-15T02:23:21.720836Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"<font size=4>This code block splits the dataset into training and validation sets, preprocesses the image and label data, and ensures the label tensors have the correct dimensions for use in the model.<font size>","metadata":{}},{"cell_type":"code","source":"splitter = RandomSplitter(valid_pct=0.2)\ntrain_idx, valid_idx = splitter(range(len(train_x)))\n\nx_train = [train_x[i] for i in train_idx]\ny_train = [train_y[i] for i in train_idx]\nx_valid = [train_x[i] for i in valid_idx]\ny_valid = [train_y[i] for i in valid_idx]\n\nx_train = (torch.stack(x_train).float()/255).view(-1,28*28)\ny_train = tensor(y_train).view(-1,1)\nx_valid = (torch.stack(x_valid).float()/255).view(-1,28*28)\ny_valid = tensor(y_valid).view(-1,1)\n\nx_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n\ny_train = y_train.squeeze()\ny_valid = y_valid.squeeze()","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:24.096468Z","iopub.execute_input":"2023-02-15T02:23:24.097338Z","iopub.status.idle":"2023-02-15T02:23:24.573954Z","shell.execute_reply.started":"2023-02-15T02:23:24.097296Z","shell.execute_reply":"2023-02-15T02:23:24.572875Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"<font size=4>Combining the inputs and labels for the training dataset into tuples, and creating a list of these tuples.<font size>","metadata":{}},{"cell_type":"code","source":"training = list(zip(x_train, y_train))\nvalidation = list(zip(x_valid, y_valid))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:27.324066Z","iopub.execute_input":"2023-02-15T02:23:27.324480Z","iopub.status.idle":"2023-02-15T02:23:27.964365Z","shell.execute_reply.started":"2023-02-15T02:23:27.324436Z","shell.execute_reply":"2023-02-15T02:23:27.963001Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"<font size=4> This function calculates the cross entropy loss between the predicted values and the actual labels. First, we apply the log softmax function to the predicted values. Then we calculate the negative log likelihood loss using the actual labels. This loss penalizes the model more when it makes more confident incorrect predictions, and less when it makes less confident incorrect predictions. Lastly we return loss value. <font size>\n    ","metadata":{}},{"cell_type":"code","source":"def cross_entropy_loss(predictions, train_y):\n    log_softmax = F.log_softmax(predictions, dim=1)\n    loss = F.nll_loss(log_softmax, train_y)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:30.144163Z","iopub.execute_input":"2023-02-15T02:23:30.144744Z","iopub.status.idle":"2023-02-15T02:23:30.151933Z","shell.execute_reply.started":"2023-02-15T02:23:30.144695Z","shell.execute_reply":"2023-02-15T02:23:30.150607Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"<font size=4> This function calculates the accuracy of the predicted labels for a batch of inputs \n    by comparing them to the actual target labels. Then function first computes the predicted labels by taking the argmax over the output predictions. It then compares these predicted labels to the actual labels and returns the mean accuracy over the batch. <font size>","metadata":{}},{"cell_type":"code","source":"def batch_accuracy(preds, targets):\n    preds = torch.argmax(preds, dim=1)\n    return (preds == targets).float().mean()","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:33.335609Z","iopub.execute_input":"2023-02-15T02:23:33.336193Z","iopub.status.idle":"2023-02-15T02:23:33.343590Z","shell.execute_reply.started":"2023-02-15T02:23:33.336143Z","shell.execute_reply":"2023-02-15T02:23:33.341790Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"<font size=4> Creating PyTorch/FastAI data loaders for our training and validation sets. <font size>","metadata":{}},{"cell_type":"code","source":"dl = DataLoader(training, batch_size=256)\nvalid_dl = DataLoader(validation, batch_size=256)\ndls = DataLoaders(dl, valid_dl)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:23:36.013280Z","iopub.execute_input":"2023-02-15T02:23:36.014448Z","iopub.status.idle":"2023-02-15T02:23:36.024672Z","shell.execute_reply.started":"2023-02-15T02:23:36.014402Z","shell.execute_reply":"2023-02-15T02:23:36.023259Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(fastai.data.load.DataLoader,\n fastai.data.load.DataLoader,\n fastai.data.core.DataLoaders)"},"metadata":{}}]},{"cell_type":"markdown","source":"<font size=4> Creating feedforward neural network with ReLU activation functions <font size>","metadata":{}},{"cell_type":"code","source":"simple_net = nn.Sequential(\n    nn.Linear(28*28,100),\n    nn.ReLU(),\n    nn.Linear(100,50),\n    nn.ReLU(),\n    nn.Linear(50,10)\n   \n)","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:33:20.707800Z","iopub.execute_input":"2023-02-15T02:33:20.708420Z","iopub.status.idle":"2023-02-15T02:33:20.721821Z","shell.execute_reply.started":"2023-02-15T02:33:20.708368Z","shell.execute_reply":"2023-02-15T02:33:20.720520Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"<font size=4> Creating FastAI learner with \"our\" neural network, loss function and batch accuracy. Using SGD for optimizing.<font size>","metadata":{}},{"cell_type":"code","source":"learn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=cross_entropy_loss, metrics=batch_accuracy,)","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:33:25.699643Z","iopub.execute_input":"2023-02-15T02:33:25.700113Z","iopub.status.idle":"2023-02-15T02:33:25.707553Z","shell.execute_reply.started":"2023-02-15T02:33:25.700076Z","shell.execute_reply":"2023-02-15T02:33:25.706164Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"<font size=4> And then training with 0.1 lr <font size>\n    ","metadata":{}},{"cell_type":"code","source":"learn.fit(20, 0.1)","metadata":{"execution":{"iopub.status.busy":"2023-02-15T02:33:28.583481Z","iopub.execute_input":"2023-02-15T02:33:28.584746Z","iopub.status.idle":"2023-02-15T02:33:52.834871Z","shell.execute_reply.started":"2023-02-15T02:33:28.584688Z","shell.execute_reply":"2023-02-15T02:33:52.833496Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>batch_accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.670606</td>\n      <td>0.601179</td>\n      <td>0.794167</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.374279</td>\n      <td>0.445963</td>\n      <td>0.859667</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.309616</td>\n      <td>0.373854</td>\n      <td>0.882750</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.272211</td>\n      <td>0.324112</td>\n      <td>0.899000</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.243279</td>\n      <td>0.281811</td>\n      <td>0.913917</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.218369</td>\n      <td>0.248300</td>\n      <td>0.926583</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.197272</td>\n      <td>0.223467</td>\n      <td>0.933500</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.179384</td>\n      <td>0.204190</td>\n      <td>0.937250</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.164212</td>\n      <td>0.187359</td>\n      <td>0.943333</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.151023</td>\n      <td>0.173701</td>\n      <td>0.947917</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.139607</td>\n      <td>0.162122</td>\n      <td>0.951167</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.129605</td>\n      <td>0.151907</td>\n      <td>0.954167</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.120550</td>\n      <td>0.142938</td>\n      <td>0.956750</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.112518</td>\n      <td>0.135233</td>\n      <td>0.958667</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.105157</td>\n      <td>0.128735</td>\n      <td>0.960250</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.098466</td>\n      <td>0.123318</td>\n      <td>0.962417</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.092389</td>\n      <td>0.118607</td>\n      <td>0.963583</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.086778</td>\n      <td>0.114256</td>\n      <td>0.964333</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.081637</td>\n      <td>0.110838</td>\n      <td>0.965833</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.076934</td>\n      <td>0.107942</td>\n      <td>0.967167</td>\n      <td>00:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"<font size=4> The goal of this small project was to gain a deeper understanding of key concepts in deep learning such as tensors and data, Stochastic Gradient Descent (SGD), loss functions, neural networks, and batches.The original plan for this project was to build everything from scratch, but after experimenting with various methods and working with small batches, i found it more interesting (and increasing in my learning process) to do few things and mostly just use fastai library. Especially understand those few functions, and test different parameters and approaches, which was way easier with FastAI/Pytorch library. Not using test data for final accuracy, because it was not point here. Mainly using material from fastbook [https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb](http://) <font size>","metadata":{}}]}